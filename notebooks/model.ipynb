{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/01 12:22:42 INFO mlflow.tracking.fluent: Experiment with name 'Credit_Risk_Modeling_20250701_122239' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/tsega/OneDrive/Documents/credit-scoring/artifacts/7', creation_time=1751361762459, experiment_id='7', last_update_time=1751361762459, lifecycle_stage='active', name='Credit_Risk_Modeling_20250701_122239', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # adjust if needed\n",
    "experiment_name = f\"Credit_Risk_Modeling_{datetime.now():%Y%m%d_%H%M%S}\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary_total</th>\n",
       "      <th>is_high_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CustomerId_1</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>-10000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CustomerId_10</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>-10000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CustomerId_1001</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CustomerId_1002</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>4225.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CustomerId_1003</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CustomerId  recency  frequency  monetary_total  is_high_risk\n",
       "0     CustomerId_1       84          1        -10000.0             0\n",
       "1    CustomerId_10       84          1        -10000.0             0\n",
       "2  CustomerId_1001       90          5         20000.0             0\n",
       "3  CustomerId_1002       26         11          4225.0             0\n",
       "4  CustomerId_1003       12          6         20000.0             0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = Path(\"../data/processed/modeling_data.csv\")  # <-- change if necessary\n",
    "df = pd.read_csv(data_path)\n",
    "display(df.head())\n",
    "assert \"is_high_risk\" in df.columns, \"Column 'is_high_risk' missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2993, 3) (749, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocessing\n",
    "df = df.copy()\n",
    "df.drop(columns=[c for c in [\"CustomerId\", \"TransactionId\"] if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "y = df[\"is_high_risk\"].astype(int)\n",
    "X = df.drop(columns=[\"is_high_risk\"]).select_dtypes(include=[\"number\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_params = {\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"learning_rate\": [0.01, 0.1],\n",
    "            \"max_depth\": [3, 5],\n",
    "        },\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsega\\OneDrive\\Documents\\credit-scoring\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tsega\\OneDrive\\Documents\\credit-scoring\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\tsega\\OneDrive\\Documents\\credit-scoring\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "2025/07/01 12:29:04 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/01 12:29:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'CreditRisk_RandomForest' already exists. Creating a new version of this model...\n",
      "2025/07/01 12:29:12 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: CreditRisk_RandomForest, version 2\n",
      "Created version '2' of model 'CreditRisk_RandomForest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest Evaluation Metrics:\n",
      "• Accuracy:    0.9987\n",
      "• Precision:   0.0000\n",
      "• Recall:      0.0000\n",
      "• F1 Score:    0.0000\n",
      "• ROC-AUC:     1.0000\n",
      "• CV Score:    nan\n",
      "🏃 View run RandomForest_20250701_122900221200_388 at: http://localhost:5000/#/experiments/7/runs/6754b2244236426e9dd7c8197fe59529\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsega\\OneDrive\\Documents\\credit-scoring\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tsega\\OneDrive\\Documents\\credit-scoring\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "2025/07/01 12:29:14 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/01 12:29:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'CreditRisk_GradientBoosting' already exists. Creating a new version of this model...\n",
      "2025/07/01 12:29:19 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: CreditRisk_GradientBoosting, version 2\n",
      "Created version '2' of model 'CreditRisk_GradientBoosting'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GradientBoosting Evaluation Metrics:\n",
      "• Accuracy:    0.9987\n",
      "• Precision:   0.5000\n",
      "• Recall:      1.0000\n",
      "• F1 Score:    0.6667\n",
      "• ROC-AUC:     0.9993\n",
      "• CV Score:    nan\n",
      "🏃 View run GradientBoosting_20250701_122913581239_475 at: http://localhost:5000/#/experiments/7/runs/2c9ebd829eb94a51b1afe0148e1a030a\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsega\\OneDrive\\Documents\\credit-scoring\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tsega\\OneDrive\\Documents\\credit-scoring\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "2025/07/01 12:29:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/01 12:29:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'CreditRisk_LogisticRegression' already exists. Creating a new version of this model...\n",
      "2025/07/01 12:29:24 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: CreditRisk_LogisticRegression, version 2\n",
      "Created version '2' of model 'CreditRisk_LogisticRegression'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression Evaluation Metrics:\n",
      "• Accuracy:    0.9973\n",
      "• Precision:   0.3333\n",
      "• Recall:      1.0000\n",
      "• F1 Score:    0.5000\n",
      "• ROC-AUC:     1.0000\n",
      "• CV Score:    nan\n",
      "🏃 View run LogisticRegression_20250701_122919866094_738 at: http://localhost:5000/#/experiments/7/runs/39704e328845496480c2c7493889efc1\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/7\n",
      "\n",
      "=== Model Performance Comparison ===\n",
      "\n",
      "RandomForest:\n",
      "  ROC-AUC:   1.0000\n",
      "  Accuracy:  0.9987\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1:        0.0000\n",
      "\n",
      "GradientBoosting:\n",
      "  ROC-AUC:   0.9993\n",
      "  Accuracy:  0.9987\n",
      "  Precision: 0.5000\n",
      "  Recall:    1.0000\n",
      "  F1:        0.6667\n",
      "\n",
      "LogisticRegression:\n",
      "  ROC-AUC:   1.0000\n",
      "  Accuracy:  0.9973\n",
      "  Precision: 0.3333\n",
      "  Recall:    1.0000\n",
      "  F1:        0.5000\n",
      "\n",
      "Best model overall: RandomForest (ROC-AUC: 1.0000)\n",
      "🏃 View run Main_Experiment at: http://localhost:5000/#/experiments/7/runs/a76764eb20e94e3bb0c8c77448734598\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/7\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "best_models = {}\n",
    "best_scores = {}\n",
    "all_metrics = {}  # To store metrics for all models\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Main_Experiment\") as parent_run:\n",
    "    mlflow.log_param(\"train_size\", len(X_train))\n",
    "    mlflow.log_param(\"test_size\", len(X_test))\n",
    "    mlflow.log_param(\"positive_class_ratio\", y.mean())\n",
    "\n",
    "    for name, cfg in models_params.items():\n",
    "        # Create unique run ID with timestamp and random suffix\n",
    "        run_id = f\"{name}_{datetime.now().strftime('%Y%m%d_%H%M%S%f')}_{np.random.randint(100,999)}\"\n",
    "        \n",
    "        with mlflow.start_run(nested=True, run_name=run_id):\n",
    "            # Small delay to prevent timestamp collisions\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            gs = GridSearchCV(\n",
    "                cfg[\"model\"],\n",
    "                cfg[\"params\"],\n",
    "                cv=cv,\n",
    "                scoring=\"roc_auc\",\n",
    "                n_jobs=-1,\n",
    "                verbose=0,\n",
    "            )\n",
    "            gs.fit(X_train, y_train)\n",
    "\n",
    "            preds = gs.predict(X_test)\n",
    "            probs = gs.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            metrics = {\n",
    "                \"accuracy\": accuracy_score(y_test, preds),\n",
    "                \"precision\": precision_score(y_test, preds),\n",
    "                \"recall\": recall_score(y_test, preds),\n",
    "                \"f1\": f1_score(y_test, preds),\n",
    "                \"roc_auc\": roc_auc_score(y_test, probs),\n",
    "                \"best_cv_score\": gs.best_score_,\n",
    "            }\n",
    "            \n",
    "            # Store metrics for display\n",
    "            all_metrics[name] = metrics\n",
    "\n",
    "            # Generate unique metric prefix\n",
    "            metric_prefix = f\"{datetime.now().strftime('%f')}_\"\n",
    "\n",
    "            # Log model FIRST with simple artifact_path\n",
    "            mlflow.sklearn.log_model(\n",
    "                gs.best_estimator_,\n",
    "                artifact_path=name.lower(),  # No slashes in artifact_path\n",
    "                registered_model_name=f\"CreditRisk_{name}\",\n",
    "            )\n",
    "\n",
    "            # Then log parameters and metrics with unique prefixes\n",
    "            mlflow.log_params({f\"{name}_{k}\": v for k, v in gs.best_params_.items()})\n",
    "            \n",
    "            # Log metrics with unique prefixes and retries\n",
    "            for metric, value in metrics.items():\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        mlflow.log_metric(f\"{metric_prefix}{name}_{metric}\", value)\n",
    "                        time.sleep(0.05)\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        if attempt == 2:\n",
    "                            raise\n",
    "                        time.sleep(1)\n",
    "\n",
    "            best_models[name] = gs.best_estimator_\n",
    "            best_scores[name] = metrics[\"roc_auc\"]\n",
    "\n",
    "            # Display metrics in a readable format\n",
    "            print(f\"\\n{name} Evaluation Metrics:\")\n",
    "            print(f\"• Accuracy:    {metrics['accuracy']:.4f}\")\n",
    "            print(f\"• Precision:   {metrics['precision']:.4f}\")\n",
    "            print(f\"• Recall:      {metrics['recall']:.4f}\")\n",
    "            print(f\"• F1 Score:    {metrics['f1']:.4f}\")\n",
    "            print(f\"• ROC-AUC:     {metrics['roc_auc']:.4f}\")\n",
    "            print(f\"• CV Score:    {metrics['best_cv_score']:.4f}\")\n",
    "\n",
    "    # Identify and log best model\n",
    "    best_name = max(best_scores, key=best_scores.get)\n",
    "    mlflow.set_tag(\"best_model\", best_name)\n",
    "    mlflow.log_metric(\"best_overall_roc_auc\", best_scores[best_name])\n",
    "    \n",
    "    # Display final comparison\n",
    "    print(\"\\n=== Model Performance Comparison ===\")\n",
    "    for name, metrics in all_metrics.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1:        {metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest model overall: {best_name} (ROC-AUC: {best_scores[best_name]:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
